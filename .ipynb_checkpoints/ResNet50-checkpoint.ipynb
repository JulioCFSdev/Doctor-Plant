{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1ab528-b688-4ead-be8a-507b5158d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "import multiprocessing\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfed36a8-83e8-4beb-adfb-927a6fe4ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "\n",
    "sdir=r'archive\\Image Data base\\Image Data base'\n",
    "min_samples= 300 # set limit for minimum images a class must have to be included in the dataframe\n",
    "filepaths = []\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2646897-3d99-4815-87a1-64805d4e330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(sdir, min_samples, filepaths, labels):\n",
    "    classlist=os.listdir(sdir)   \n",
    "    for klass in classlist:\n",
    "        classpath=os.path.join(sdir, klass)\n",
    "        flist=os.listdir(classpath)\n",
    "        if len(flist) >= min_samples:\n",
    "            for f in flist:\n",
    "                fpath=os.path.join(classpath,f)\n",
    "                filepaths.append(fpath)\n",
    "                labels.append(klass)\n",
    "        else:\n",
    "            print('class ', klass, ' has only', len(flist), ' samples and will not be included in dataframe')\n",
    "\n",
    "def holdout(filepaths, labels):\n",
    "    Fseries=pd.Series(filepaths, name='filepaths')\n",
    "    Lseries=pd.Series(labels, name='labels')        \n",
    "    df=pd.concat([Fseries, Lseries], axis=1)\n",
    "\n",
    "\n",
    "    train_df, dummy_df=train_test_split(df, train_size=.7, shuffle=True, random_state=123, stratify=df['labels'])\n",
    "\n",
    "    valid_df, test_df=train_test_split(dummy_df, train_size=.33, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
    "\n",
    "    print('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
    "    return (train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396b2dc0-5ceb-4758-8c75-bc362263b76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  Bacterial leaf blight in rice leaf  has only 120  samples and will not be included in dataframe\n",
      "class  Brown spot in rice leaf  has only 120  samples and will not be included in dataframe\n",
      "class  cabbage looper  has only 234  samples and will not be included in dataframe\n",
      "class  Cercospora leaf spot  has only 189  samples and will not be included in dataframe\n",
      "class  Garlic  has only 147  samples and will not be included in dataframe\n",
      "class  ginger  has only 135  samples and will not be included in dataframe\n",
      "class  healthy tea leaf  has only 222  samples and will not be included in dataframe\n",
      "class  Leaf smut in rice leaf  has only 120  samples and will not be included in dataframe\n",
      "class  lemon canker  has only 183  samples and will not be included in dataframe\n",
      "class  Nitrogen deficiency in plant  has only 33  samples and will not be included in dataframe\n",
      "class  onion  has only 60  samples and will not be included in dataframe\n",
      "class  potassium deficiency in plant  has only 54  samples and will not be included in dataframe\n",
      "class  potato crop  has only 120  samples and will not be included in dataframe\n",
      "class  potato hollow heart  has only 180  samples and will not be included in dataframe\n",
      "class  Sogatella rice  has only 78  samples and will not be included in dataframe\n",
      "class  tomato canker  has only 57  samples and will not be included in dataframe\n",
      "class  Waterlogging in plant  has only 21  samples and will not be included in dataframe\n"
     ]
    }
   ],
   "source": [
    "filter_data(sdir, min_samples, filepaths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2db89d-f5fa-4a2d-ac41-0eb97c96a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df lenght:  138314   test_df length:  39717   valid_df length:  19561\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df, test_df = holdout(filepaths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f0acc3-cad7-4fbb-97b2-822cd9cd8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes in the dataset is:  41\n",
      "            CLASS               IMAGE COUNT \n",
      "       Apple Apple scab            4234     \n",
      "       Apple Black rot             4175     \n",
      "    Apple Cedar apple rust         1848     \n",
      "        Apple healthy              2764     \n",
      "     Blight in corn Leaf           2407     \n",
      "      Blueberry healthy            2524     \n",
      "Cherry (including sour) Powdery mildew     1768     \n",
      "Cherry (including_sour) healthy     1436     \n",
      "   Common Rust in corn Leaf        2743     \n",
      "     Corn (maize) healthy          1953     \n",
      "       Grape Black rot             7930     \n",
      "   Grape Esca Black Measles        9299     \n",
      "Grape Leaf blight Isariopsis Leaf Spot     7232     \n",
      "        Grape healthy               712     \n",
      " Gray Leaf Spot in corn Leaf       1205     \n",
      "Orange Haunglongbing Citrus greening     37010    \n",
      "        Peach healthy               605     \n",
      "  Pepper bell Bacterial spot       2092     \n",
      "     Pepper bell healthy           2484     \n",
      "     Potato Early blight           2100     \n",
      "      Potato Late blight           2100     \n",
      "        Potato healthy              256     \n",
      "      Raspberry healthy             624     \n",
      "       Soybean healthy             8551     \n",
      "    Strawberry Leaf scorch         1865     \n",
      "      Strawberry healthy            767     \n",
      "    Tomato Bacterial spot          4467     \n",
      "     Tomato Early blight           2100     \n",
      "      Tomato Late blight           4009     \n",
      "       Tomato Leaf Mold            1999     \n",
      "  Tomato Septoria leaf spot        3719     \n",
      "Tomato Spider mites Two spotted spider mite     3520     \n",
      "      Tomato Target Spot           2948     \n",
      "  Tomato Tomato mosaic virus        783     \n",
      "        Tomato healthy             2673     \n",
      "      algal leaf in tea             237     \n",
      "      anthracnose in tea            210     \n",
      "     bird eye spot in tea           210     \n",
      "     brown blight in tea            237     \n",
      "          corn crop                 218     \n",
      "     red leaf spot in tea           300     \n"
     ]
    }
   ],
   "source": [
    "# get the number of classes and the images count for each class in train_df\n",
    "classes=sorted(list(train_df['labels'].unique()))\n",
    "class_count = len(classes)\n",
    "print('The number of classes in the dataset is: ', class_count)\n",
    "groups=train_df.groupby('labels')\n",
    "print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "countlist=[]\n",
    "classlist=[]\n",
    "\n",
    "for label in sorted(list(train_df['labels'].unique())):\n",
    "    group=groups.get_group(label)\n",
    "    countlist.append(len(group))\n",
    "    classlist.append(label)\n",
    "    print('{0:^30s} {1:^13s}'.format(label, str(len(group))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76adc2f8-0cc0-4d50-989d-9388e650d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange Haunglongbing Citrus greening  has the most images=  37010   anthracnose in tea  has the least images=  210\n"
     ]
    }
   ],
   "source": [
    "# get the classes with the minimum and maximum number of train images\n",
    "max_value=np.max(countlist)\n",
    "max_index=countlist.index(max_value)\n",
    "max_class=classlist[max_index]\n",
    "min_value=np.min(countlist)\n",
    "min_index=countlist.index(min_value)\n",
    "min_class=classlist[min_index]\n",
    "\n",
    "print(max_class, ' has the most images= ',max_value, ' ', min_class, ' has the least images= ', min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09951437-c30b-4c5c-8a9f-85923d9ad099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average height=  256  average width=  259 aspect ratio=  0.9884526558891455\n"
     ]
    }
   ],
   "source": [
    "# lets get the average height and width of a sample of the train images\n",
    "ht=0\n",
    "wt=0\n",
    "\n",
    "# select 100 random samples of train_df\n",
    "train_df_sample=train_df.sample(n=100, random_state=123,axis=0)\n",
    "\n",
    "for i in range (len(train_df_sample)):\n",
    "    fpath=train_df_sample['filepaths'].iloc[i]\n",
    "    img=plt.imread(fpath)\n",
    "    shape=img.shape\n",
    "    ht += shape[0]\n",
    "    wt += shape[1]\n",
    "\n",
    "print('average height= ', ht//100, ' average width= ', wt//100, 'aspect ratio= ', ht/wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3dfc0fc-29bf-4927-8c7f-8b748006f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 138314 validated image filenames belonging to 41 classes.\n",
      "Found 19561 validated image filenames belonging to 41 classes.\n"
     ]
    }
   ],
   "source": [
    "# Definindo tamanho do batch e tamanho da imagem\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Criando geradores de imagens para treinamento e validação\n",
    "train_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True, \n",
    "    rotation_range=20, \n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2, \n",
    "    zoom_range=.2\n",
    ").flow_from_dataframe(\n",
    "    train_df, \n",
    "    x_col='filepaths', \n",
    "    y_col='labels', \n",
    "    target_size=img_size,\n",
    "    class_mode='categorical', \n",
    "    color_mode='rgb', \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_gen = ImageDataGenerator().flow_from_dataframe(\n",
    "    valid_df, \n",
    "    x_col='filepaths', \n",
    "    y_col='labels', \n",
    "    target_size=img_size,\n",
    "    class_mode='categorical', \n",
    "    color_mode='rgb', \n",
    "    shuffle=False, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3e3e7a-9af5-4da7-a9f1-892059e19729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39717 validated image filenames belonging to 41 classes.\n",
      "test batch size:  32   test steps:  1241  number of classes :  41\n"
     ]
    }
   ],
   "source": [
    "# Calculando tamanho do batch para conjunto de teste\n",
    "length = len(test_df)\n",
    "test_batch_size = 32\n",
    "test_steps = int(length / test_batch_size)\n",
    "\n",
    "# Criando gerador de imagens para teste\n",
    "test_gen = ImageDataGenerator().flow_from_dataframe(\n",
    "    test_df, \n",
    "    x_col='filepaths', \n",
    "    y_col='labels', \n",
    "    target_size=img_size,\n",
    "    class_mode='categorical', \n",
    "    color_mode='rgb', \n",
    "    shuffle=False, \n",
    "    batch_size=test_batch_size\n",
    ")\n",
    "\n",
    "# Obtendo informações relevantes dos geradores\n",
    "classes = list(train_gen.class_indices.keys())\n",
    "class_indices = list(train_gen.class_indices.values())\n",
    "class_count = len(classes)\n",
    "labels = test_gen.labels\n",
    "\n",
    "\n",
    "# Imprimindo informações úteis\n",
    "print('test batch size: ', test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b36b59-19cf-49c5-a57b-b0a3523835d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4323/4323 [==============================] - 437s 99ms/step - loss: 0.3927 - accuracy: 0.9012 - val_loss: 0.1812 - val_accuracy: 0.9519\n",
      "Epoch 2/300\n",
      "4323/4323 [==============================] - 388s 90ms/step - loss: 0.2482 - accuracy: 0.9315 - val_loss: 0.1623 - val_accuracy: 0.9562\n",
      "Epoch 3/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.2106 - accuracy: 0.9411 - val_loss: 0.1308 - val_accuracy: 0.9638\n",
      "Epoch 4/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1885 - accuracy: 0.9464 - val_loss: 0.0969 - val_accuracy: 0.9716\n",
      "Epoch 5/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1770 - accuracy: 0.9504 - val_loss: 0.1240 - val_accuracy: 0.9674\n",
      "Epoch 6/300\n",
      "4323/4323 [==============================] - 394s 91ms/step - loss: 0.1599 - accuracy: 0.9551 - val_loss: 0.0929 - val_accuracy: 0.9726\n",
      "Epoch 7/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1534 - accuracy: 0.9566 - val_loss: 0.1318 - val_accuracy: 0.9668\n",
      "Epoch 8/300\n",
      "4323/4323 [==============================] - 386s 89ms/step - loss: 0.1448 - accuracy: 0.9583 - val_loss: 0.0924 - val_accuracy: 0.9744\n",
      "Epoch 9/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1432 - accuracy: 0.9589 - val_loss: 0.0977 - val_accuracy: 0.9716\n",
      "Epoch 10/300\n",
      "4323/4323 [==============================] - 386s 89ms/step - loss: 0.1341 - accuracy: 0.9611 - val_loss: 0.0937 - val_accuracy: 0.9751\n",
      "Epoch 11/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1295 - accuracy: 0.9629 - val_loss: 0.0787 - val_accuracy: 0.9783\n",
      "Epoch 12/300\n",
      "4323/4323 [==============================] - 387s 89ms/step - loss: 0.1279 - accuracy: 0.9628 - val_loss: 0.0886 - val_accuracy: 0.9756\n",
      "Epoch 13/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1231 - accuracy: 0.9645 - val_loss: 0.0714 - val_accuracy: 0.9800\n",
      "Epoch 14/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1168 - accuracy: 0.9658 - val_loss: 0.0677 - val_accuracy: 0.9810\n",
      "Epoch 15/300\n",
      "4323/4323 [==============================] - 384s 89ms/step - loss: 0.1181 - accuracy: 0.9659 - val_loss: 0.0814 - val_accuracy: 0.9775\n",
      "Epoch 16/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1156 - accuracy: 0.9666 - val_loss: 0.0861 - val_accuracy: 0.9762\n",
      "Epoch 17/300\n",
      "4323/4323 [==============================] - 387s 89ms/step - loss: 0.1137 - accuracy: 0.9674 - val_loss: 0.0705 - val_accuracy: 0.9793\n",
      "Epoch 18/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1114 - accuracy: 0.9675 - val_loss: 0.0821 - val_accuracy: 0.9768\n",
      "Epoch 19/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1091 - accuracy: 0.9677 - val_loss: 0.0786 - val_accuracy: 0.9785\n",
      "Epoch 20/300\n",
      "4323/4323 [==============================] - 386s 89ms/step - loss: 0.1099 - accuracy: 0.9677 - val_loss: 0.0605 - val_accuracy: 0.9821\n",
      "Epoch 21/300\n",
      "4323/4323 [==============================] - 394s 91ms/step - loss: 0.1046 - accuracy: 0.9697 - val_loss: 0.0729 - val_accuracy: 0.9793\n",
      "Epoch 22/300\n",
      "4323/4323 [==============================] - 412s 95ms/step - loss: 0.1065 - accuracy: 0.9693 - val_loss: 0.0719 - val_accuracy: 0.9788\n",
      "Epoch 23/300\n",
      "4323/4323 [==============================] - 385s 89ms/step - loss: 0.1020 - accuracy: 0.9700 - val_loss: 0.0595 - val_accuracy: 0.9816\n",
      "Epoch 24/300\n",
      "4323/4323 [==============================] - 419s 97ms/step - loss: 0.1029 - accuracy: 0.9699 - val_loss: 0.0736 - val_accuracy: 0.9786\n",
      "Epoch 25/300\n",
      "4323/4323 [==============================] - 413s 96ms/step - loss: 0.1023 - accuracy: 0.9706 - val_loss: 0.0682 - val_accuracy: 0.9810\n",
      "Epoch 26/300\n",
      "4323/4323 [==============================] - 418s 97ms/step - loss: 0.1026 - accuracy: 0.9698 - val_loss: 0.0891 - val_accuracy: 0.9766\n",
      "Epoch 27/300\n",
      "2332/4323 [===============>..............] - ETA: 2:50 - loss: 0.0975 - accuracy: 0.9709"
     ]
    }
   ],
   "source": [
    "img_shape=(img_size[0], img_size[1], 3)\n",
    "epocas = 300\n",
    "\n",
    "# Define o modelo base\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights='imagenet', \n",
    "    include_top=False,\n",
    "    input_shape=img_shape,\n",
    "    pooling='max')\n",
    "\n",
    "# Adiciona camadas adicionais para adaptação ao conjunto de dados\n",
    "x = base_model.output\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(rate=.4, seed=123)(x)\n",
    "\n",
    "\n",
    "predictions = Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "# Define o modelo final\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Congela as camadas do modelo base para que não sejam treinadas durante o treinamento\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compila o modelo com uma função de perda e um otimizador\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#define o earlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=(epocas*0.1), verbose=1)\n",
    "\n",
    "# Treina o modelo\n",
    "trained_model = model.fit(train_gen, validation_data=valid_gen, epochs=300, callbacks=[earlystop], workers = workers)\n",
    "\n",
    "# Salva o modelo treinado em um arquivo local\n",
    "model.save('ResNet50_model_trained.h5')\n",
    "\n",
    "# Avalia o modelo no conjunto de teste\n",
    "y_predict = model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583a7d9-3fa1-48db-93a2-1755fa95d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc7109-bd93-4a85-987b-2b4e0dca1f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
